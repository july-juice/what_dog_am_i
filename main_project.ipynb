{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a67ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Packages for this code:\n",
    "\n",
    "# python3 -m pip install --upgrade numpy opencv-python tensorflow imageio pillow jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d7c0d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "cascade_path = pathlib.Path(cv2.__file__).parent.absolute() / \"data/haarcascade_frontalface_default.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb3b576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = cv2.CascadeClassifier(str(cascade_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9040dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Button Region\n",
    "\n",
    "button_location = (50,50,300,120)\n",
    "quitbuttonpressed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75d78457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mouse Click Detection Function and change boolean variable\n",
    "\n",
    "def mouse_click(event,x,y,flags,param):\n",
    "    global quitbuttonpressed\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        x1,y1,x2,y2 = button_location\n",
    "        if x1 <= x <= x2 and y1 <= y <= y2:\n",
    "            quitbuttonpressed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "429d3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow(\"Faces\")\n",
    "cv2.setMouseCallback(\"Faces\", mouse_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "962f25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera Load\n",
    "\n",
    "camera = cv2.VideoCapture(1) # Either 0 or 1, dependent on whther a phone is connected to the computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3eb3fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dog images\n",
    "\n",
    "dogimages = {\n",
    "    \"tongue_out\": cv2.imread(\"dogimages/tongue_out.jpg\"),\n",
    "    \"eyesclosed\": cv2.imread(\"dogimages/eyesclosed.jpg\"),\n",
    "    \"perplexed\": cv2.imread(\"dogimages/perplexed.JPG\"),\n",
    "    \"scream\": cv2.imread(\"dogimages/scream.jpg\"),\n",
    "    \"smile\": cv2.imread(\"dogimages/smile.jpg\")            \n",
    "}\n",
    "\n",
    "for doggy in dogimages:\n",
    "    dogimages[doggy] = cv2.resize(dogimages[doggy], (480,480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to either v1,2 or 3 depending on which model to test.\n",
    "\n",
    "model = load_model(\"CustomNetworkv1.keras\")\n",
    "\n",
    "\n",
    "classes = [\"eyesclosed\",\n",
    "           \"scream\",\n",
    "           \"tongue_out\"]\n",
    "\n",
    "# Redundant, however the for flow of code it works fine.\n",
    "model_to_dog = {\n",
    "    'eyesclosed': 'eyesclosed',\n",
    "    'scream': 'scream',\n",
    "    'tongue_out': 'tongue_out'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d13299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize detected faces to 224x224, preprocess the image data\n",
    "\n",
    "def prepare_face(face_img):\n",
    "    resized = cv2.resize(face_img, (224,224))\n",
    "    img_array = np.expand_dims(resized,axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "332da304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for centred text\n",
    "\n",
    "def _put_centered_text(img, text, color=(255,255,255), scale=1.1, thickness=2):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    (tw, th), baseline = cv2.getTextSize(text, font, scale, thickness)\n",
    "    x = (img.shape[1] - tw) // 2\n",
    "    y = (img.shape[0] + th) // 2\n",
    "    cv2.putText(img, text, (x, y), font, scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "# Fade out function\n",
    "\n",
    "def _fade_out(img, steps=30, delay=30):\n",
    "    for alpha in np.linspace(1.0, 0.0, steps):\n",
    "        frame = cv2.convertScaleAbs(img, alpha=alpha, beta=0)\n",
    "        cv2.imshow(\"Faces\", frame)\n",
    "        if cv2.waitKey(delay) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Function to play intro gif\n",
    "\n",
    "def _play_gif(gif_path, duration_sec=2.0, size=(480,960), window=\"Faces\"):\n",
    "    import time\n",
    "    try:\n",
    "        import imageio.v2 as imageio\n",
    "    except ImportError:\n",
    "        return None\n",
    "    last_bgr = None\n",
    "    try:\n",
    "        reader = imageio.get_reader(gif_path)\n",
    "    except Exception:\n",
    "        return None\n",
    "    t_end = time.time() + duration_sec\n",
    "    try:\n",
    "        while time.time() < t_end:\n",
    "            for frame in reader:\n",
    "                if time.time() >= t_end:\n",
    "                    break\n",
    "                # Convert RGB/RGBA -> BGR\n",
    "                if frame.ndim == 3 and frame.shape[2] == 4:\n",
    "                    bgr = cv2.cvtColor(frame, cv2.COLOR_RGBA2BGR)\n",
    "                else:\n",
    "                    bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                bgr = cv2.resize(bgr, size)\n",
    "                last_bgr = bgr\n",
    "                cv2.imshow(window, bgr)\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "    finally:\n",
    "        try:\n",
    "            reader.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return last_bgr\n",
    "\n",
    "# Animated gif, fade to black\n",
    "base = np.zeros((480, 960, 3), dtype=np.uint8)\n",
    "last_frame = _play_gif(\"intrologoanimated.gif\", duration_sec=2.0, size=(960,480), window=\"Faces\")\n",
    "# if the gif isn't available, use a fallback\n",
    "if last_frame is None:\n",
    "    fallback = base.copy()\n",
    "    _put_centered_text(fallback, \"what dog am i\", color=(255,255,255), scale=1.2, thickness=2)\n",
    "    cv2.imshow(\"Faces\", fallback)\n",
    "    cv2.waitKey(2000)\n",
    "    last_frame = fallback\n",
    "_fade_out(last_frame)\n",
    "\n",
    "# Confirmation that camera is working\n",
    "camera_ok = camera.isOpened()\n",
    "status = base.copy()\n",
    "if camera_ok:\n",
    "    _put_centered_text(status, \"Camera working!\", color=(0,255,0), scale=1.0, thickness=2)\n",
    "else:\n",
    "    _put_centered_text(status, \"Camera not working!\", color=(0,0,255), scale=1.0, thickness=2)\n",
    "\n",
    "cv2.imshow(\"Faces\", status)\n",
    "cv2.waitKey(1500)\n",
    "_fade_out(status)\n",
    "\n",
    "if not camera_ok:\n",
    "    raise RuntimeError(\"Camera isn't working!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a4048bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret,frame = camera.read()\n",
    "    if not ret or frame is None:\n",
    "        print(\"Frame not captured.\")\n",
    "        continue\n",
    "    \n",
    "\n",
    "    height = 480\n",
    "\n",
    "    h,w,c = frame.shape\n",
    "    min_dim = min(h,w)\n",
    "    start_x = w//2 - min_dim//2\n",
    "    start_y = h//2 - min_dim//2\n",
    "\n",
    "    square_frame = frame[start_y:start_y+min_dim, start_x:start_x+min_dim]\n",
    "    display_frame = cv2.resize(square_frame, (480,480))\n",
    "\n",
    "    gray = cv2.cvtColor(display_frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = clf.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor = 1.05,\n",
    "        minNeighbors=5,\n",
    "        minSize=(10,10),\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "\n",
    "    # If a face is detected, classify it and display the dog images based off the classifier\n",
    "    \n",
    "    if len(faces) > 0:\n",
    "        faces_sorted = sorted(faces, key=lambda b: b[2]*b[3], reverse=True)\n",
    "        x,y,wf,hf = faces_sorted[0]\n",
    "\n",
    "        cv2.rectangle(display_frame ,(x,y),(x+wf, y+hf),(255,255,0),2)\n",
    "\n",
    "        face_roi = display_frame[y:y+hf, x:x+wf]\n",
    "        input_array = prepare_face(face_roi)\n",
    "        preds = model.predict(input_array, verbose = 0)\n",
    "        pred_class = classes[np.argmax(preds)]\n",
    "        current_dog = dogimages[model_to_dog[pred_class]]\n",
    "    else:\n",
    "        current_dog = dogimages[\"perplexed\"]\n",
    "\n",
    "    # Draw quit button\n",
    "\n",
    "    x1, y1, x2, y2 = button_location\n",
    "    cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0,0,255),-1)\n",
    "    cv2.putText(display_frame, \"Press to Quit\", (x1 + 20, y1  + 45),\n",
    "                 cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255,255,255),2)\n",
    "    \n",
    "    combined = np.hstack((display_frame,current_dog))\n",
    "    cv2.imshow(\"Faces\", combined)\n",
    "\n",
    "    if quitbuttonpressed or cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
